{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_squad",
      "provenance": [],
      "collapsed_sections": [
        "qDIlHd5Tos6C",
        "tRbKrFYoo8e8"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XYlThAmGZg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7806ddd8-81d4-402f-e1ff-d0b9a03b9a80"
      },
      "source": [
        "!pip install sentencepiece\n",
        "#!pip install tf-models-official\n",
        "!pip install tf-models-nightly # better to install the version in development\n",
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting tf-models-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a7/ed8d6404909f01a6b4494f5ba941bccc49283a26d6fa2f8ecae6d767cbfe/tf_models_nightly-2.2.0.dev20200613-py2.py3-none-any.whl (814kB)\n",
            "\u001b[K     |████████████████████████████████| 819kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.12.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/92/82c67b9cc9a52adf031494e83fd0fd6261992a583d18cdbd5b6153eed7a1/py-cpuinfo-6.0.0.tar.gz (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.1.91)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.29.19)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.7)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.0)\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/ad/9150ee6d50dfcf0bd672e27928f97ed1a20d74bbba3a7fd3e63ea0da1444/tf_nightly-2.3.0.dev20200613-cp36-cp36m-manylinux2010_x86_64.whl (350.2MB)\n",
            "\u001b[K     |████████████████████████████████| 350.2MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.1.0)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/7e/e94aa029999ec30951e8129fa992fecbbaffda66eba97c65d5a83f8ea96d/tensorflow_model_optimization-0.3.0-py2.py3-none-any.whl (165kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.4.1)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.6)\n",
            "Collecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client>=4.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.1.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.13)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.6.6)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.7.12)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.21.0)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2c/909a04b07360516953beaf6f66480bb6b84b817c6b300c1235bfb2901ad8/opencv_python_headless-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.18.5)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.3.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-nightly) (2.7.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub>=0.6.0->tf-models-nightly) (3.10.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/22/6fd1e108eef3ae43ee194d2089fca0221e6a8729293410ebb35fa6a119ac/tf_estimator_nightly-2.3.0.dev2020061301-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.2.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.29.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.2)\n",
            "Collecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/01/04bbab951759ef4006c89b004272e7e82341457325d50695b6baf349f48c/tb_nightly-2.3.0a20200612-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (19.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.22.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.3.1.1)\n",
            "Collecting dm-tree~=0.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/48/10fb721334810081b7e6eebeba0d12e12126c76993e8c243062d2f56a89f/dm_tree-0.1.5-cp36-cp36m-manylinux1_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (0.17.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->tf-models-nightly) (4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.4.7)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.7.2)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.6.0->tf-models-nightly) (47.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-nightly) (1.52.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-nightly) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-nightly) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-nightly) (3.1.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.16.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (1.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Building wheels for collected packages: py-cpuinfo\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-6.0.0-cp36-none-any.whl size=19782 sha256=3a1842ae1f77345292e51a5995db1768c029416449b19bcee7e6ec6c45efde11\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/f3/3c/ddcb384c30323d66d842b42f93b6cd2cbdfd1b317237042372\n",
            "Successfully built py-cpuinfo\n",
            "Installing collected packages: py-cpuinfo, tf-estimator-nightly, tb-nightly, tf-nightly, dm-tree, tensorflow-model-optimization, tf-slim, opencv-python-headless, tf-models-nightly\n",
            "Successfully installed dm-tree-0.1.5 opencv-python-headless-4.2.0.34 py-cpuinfo-6.0.0 tb-nightly-2.3.0a20200612 tensorflow-model-optimization-0.3.0 tf-estimator-nightly-2.3.0.dev2020061301 tf-models-nightly-2.2.0.dev20200613 tf-nightly-2.3.0.dev20200613 tf-slim-1.1.0\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.3.0.dev20200613)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.3.0.dev2020061301)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: tb-nightly<2.4.0a0,>=2.3.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.3.0a20200612)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.29.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (47.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdYqvxPHnI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFS9XSASHvQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a57d37c-adeb-45cc-9d10-5091f36f892a"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0-dev20200613'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv38eJzSH00S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "\n",
        "from official.nlp import optimization\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples\n",
        "from official.nlp.data.squad_lib import FeatureWriter\n",
        "from official.nlp.data.squad_lib import convert_examples_to_features\n",
        "from official.nlp.data.squad_lib import write_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjzrCZJMJN1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfdTEReCKFky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "62411936-e5ed-43c7-9e74-d01f52a0dd7b"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_PcOeemKcYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/My Drive/projects/BERT/data/squad/train-v1.1.json\",\n",
        "    \"/content/drive/My Drive/projects/BERT/data/squad/vocab.txt\",\n",
        "    \"/content/drive/My Drive/projects/BERT/data/squad/train-v1.1.tf_record\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XQGdnANLziM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.io.gfile.GFile(\"/content/drive/My Drive/projects/BERT/data/squad/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295nG2zQMYSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/projects/BERT/data/squad/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K",
        "colab_type": "text"
      },
      "source": [
        "## Squad layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjmLeQjiaOo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] \n",
        "    return unstacked_logits[0], unstacked_logits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf",
        "colab_type": "text"
      },
      "source": [
        "## Whole model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkIFb1GMT81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    def apply_bert(self, inputs):\n",
        "#        _ , sequence_output = self.bert_layer([inputs[\"input_ids\"],\n",
        "#                                               inputs[\"input_mask\"],\n",
        "#                                               inputs[\"segment_ids\"]])\n",
        "        \n",
        "        # New names for the 3 different elements of the inputs, since an update\n",
        "        # in tf_models_officials. Doesn't change anything for any other BERT\n",
        "        # usage.\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],\n",
        "                                               inputs[\"input_mask\"],\n",
        "                                               inputs[\"input_type_ids\"]])\n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a",
        "colab_type": "text"
      },
      "source": [
        "# Stage 4: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ",
        "colab_type": "text"
      },
      "source": [
        "## Creating the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEUxomxENRoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_SIZE = 88641\n",
        "NB_BATCHES_TRAIN = 2000\n",
        "BATCH_SIZE = 4\n",
        "NB_EPOCHS = 3\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pg6EKe2daFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHd5MzTdNIZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0cvDjBm_KXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6kG-HpzVK7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True)\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_C_WA5R_Cb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "284fc4ce-2ce2-4b13-b6b7-350614310565"
      },
      "source": [
        "next(iter(train_dataset_light))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2054, 3815, ...,    0,    0,    0],\n",
              "         [ 101, 2129, 2116, ...,    0,    0,    0],\n",
              "         [ 101, 1999, 2054, ...,    0,    0,    0],\n",
              "         [ 101, 2000, 3183, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([73, 25, 35, 93], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([71, 23, 35, 90], dtype=int32)>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-iE2QFC_KRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a309090f-ebfa-4736-e515-c9cb51b77c88"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/projects/BERT/ckpt_bert_squad/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl",
        "colab_type": "text"
      },
      "source": [
        "## Custom training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywelW3uaSbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t",
        "colab_type": "text"
      },
      "source": [
        "# Stage 5: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C",
        "colab_type": "text"
      },
      "source": [
        "## Prepare evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ",
        "colab_type": "text"
      },
      "source": [
        "Get the dev set in the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKl84s5WrhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/My Drive/projects/BERT/data/squad/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD",
        "colab_type": "text"
      },
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCVmIgnEo83o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/My Drive/projects/BERT/data/squad/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I",
        "colab_type": "text"
      },
      "source": [
        "Create a tokenizer for future information needs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5exQ7KwnuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo",
        "colab_type": "text"
      },
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ5GtoTxRjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_",
        "colab_type": "text"
      },
      "source": [
        "Create the eval features and the writes the tf.record file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7kGYmUwGQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZfwPEwMabx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844",
        "colab_type": "text"
      },
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqYvG5TxctF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/projects/BERT/data/squad/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8",
        "colab_type": "text"
      },
      "source": [
        "## Making the predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyckEWDbrLEX",
        "colab_type": "text"
      },
      "source": [
        "Defines a certain type of collection (like a dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWOUKaDP0H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abKVvqrRa4",
        "colab_type": "text"
      },
      "source": [
        "Returns each element of batched output at a time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScaA0SZQgQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),\n",
        "            start_logits=start_logits.numpy().tolist(),\n",
        "            end_logits=end_logits.numpy().tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLOOmnLre5C",
        "colab_type": "text"
      },
      "source": [
        "Let's make our predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqD78Xdjrvpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs\n",
        "    unique_ids = x.pop(\"unique_ids\")\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict):\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQ6kIqGriHr",
        "colab_type": "text"
      },
      "source": [
        "Write the predictions in a json file that will work with the evaluation script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLdRf7uM3Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_prediction_file = \"/content/drive/My Drive/projects/BERT/data/squad/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/My Drive/projects/BERT/data/squad/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/My Drive/projects/BERT/data/squad/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx",
        "colab_type": "text"
      },
      "source": [
        "## Home-made prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha",
        "colab_type": "text"
      },
      "source": [
        "### Input dict creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhHzx7ycXXo",
        "colab_type": "text"
      },
      "source": [
        "We will concatenate the question and the context, separated by a `[\"SEP\"]`, after tokenization, as it has been made for the training set.\n",
        "\n",
        "The important thing is that we want our answer to start with a real word and to end with a real word. The word \"ecologically\" being tokenized as `[\"ecological\", \"##ly\"]`, if the ending token is `[\"ecological\"]` we want to use \"ecologically\" as the ending word (same thing if the ending token is `[\"##ly\"]`). That is why we first split our context into words, and then into tokens, remembering to which word each token belongs to (see the `tokenize_context()` function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuNXt98Zm4u",
        "colab_type": "text"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjGoQ_wfmml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_fCe_hLC12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Tell if a chain of characters corresponds to a whitespace or not.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZA4TYnxLGVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Take a text and return a list of \"words\" by splitting it according to\n",
        "    whitespaces.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsfzt3GUNWQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Take a list of words (returned by whitespace_split()) and tokenize each word\n",
        "    one by one. Also keep track, for each new token, of its original word in the\n",
        "    text_words parameter.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)\n",
        "    return text_tok, tok_to_word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qreqEURUOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2sPGXxsYUsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Take a question and a context as strings and return a dictionary with the 3\n",
        "    elements needed for the model. Also return the context_words, the\n",
        "    context_tok to context_word ids correspondance and the length of\n",
        "    question_tok that we will need later.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # in our case the model has been\n",
        "                                                # trained to have inputs of length max 384\n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnaCZWTZpWT",
        "colab_type": "text"
      },
      "source": [
        "#### Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQlM-M8rMklA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_cE-o0U8mx",
        "colab_type": "text"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB29SCNNQ1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v5nLMNjZufe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT066rMtZ65X",
        "colab_type": "text"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcJgDa9gVShl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdGlIo5Z9IZ",
        "colab_type": "text"
      },
      "source": [
        "### Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJMBkVLd9wp",
        "colab_type": "text"
      },
      "source": [
        "We remove the ids corresponding to the question and the `[\"SEP\"]` token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwBJfsSTwRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:]\n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1u6iZAawYf",
        "colab_type": "text"
      },
      "source": [
        "First easy interpretation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJ8tp-1WvI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PHA84rarse",
        "colab_type": "text"
      },
      "source": [
        "\"Advanced\" - making sure that the start of the answer is before the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFZ2fUiRU_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9KEiFHPXXeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDBL8KKa6NP",
        "colab_type": "text"
      },
      "source": [
        "Final answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6574d1bc-2c36-4dc2-d5d1-04ba08c86d1d"
      },
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "0e672b78-9838-4300-aabd-11c9408d9058"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brg0AXkA0r74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}